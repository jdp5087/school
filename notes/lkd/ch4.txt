
scheduler divides finite resources of processing time between processes -- basis of multitasking operating system.

linux uses preemptive multitasking

time a process runs is usually predetermined, and is called a timeslice. -- linux does not employ timeslices, per se

the new scheduler of linux 2.5 was called O(1) scheduler
    great for large server loads
    shortcomings - interactive processes
    poor on desktops with interaction

Rotating Staircase Deadline scheduler introducted fair scheduling

Linux calls its scheduler the Completely Fair Scheduler -- CFS

policy is the behavior of a scheduler, and determines what runs when
       determines overall feel of a system
       responsible for optimally used processor time

processes are either:
	  I/O bound -- spends majority of time waiting for IO -- IO meaning any blockable resource
	      	       such as keyboard input, mouse, network IO, disk IO
	  processor bound -- spend much time executing code
	  	    	     tend to run until preempted
			     e.g. ssh-keygen

X Window server is both IO and processor bound
	  	    
scheduler must satisfy two conflicting goals:
	  low latency
	  high throughput

Linux favors IO bound, but is creative enough to not neglect processor bound processes

nice value - higher means lower priority
     control over proportion of the timeslice -- see nice values with ps -el

real-time priority -- default ranges from 0 to 99 higher is more priority

nice and real-time are in disjoin value spaces, with real-time having a higher priority than normal processes
     seen with ps -eo state,uid,ppid,rtprio,time,comm

timeslice -- how long a task can run until interrupted

too short of a timeslice is suboptimal because of larger switching overhead

too long and IO bound processes dont need the extra timeslice
processor bound processes want a long timeslice -- keeps cache "hot"

CFS assigns a proportion of the processor, which is a function of the load of the system

further affected by each processes nice value, which acts a weight, changing the proportion each process recieves

when a process enters an eligible state, whether or not it is run is a function of the proportion of the processor the newly runnable process has already consumed more of a proportion than the currently running process.

CFS is modular, allowing different algorithms to schedule different types of processes.
    called scheduler classes
    base scheduler iterates scheduler classes from highest to lowest, highest class that has a runnable task wins

DISCUSSION OF TRADITIONAL UNIX SCHEDULERS

mapping nice values into timeslices requires a decision about what absolute timeslice to allot each nice value

with high priority being assigned higher time slices, the proportion of processor time is totally dependant on what processes currently exist. Furthermore, since many high priority processes are IO bound, this is actually the opposite behavior of desired, becuase these high priority tasks only need a short running time, while "nice" processes are often processor bound, and prefer longer timeslices.


problem 2 -- if a process is decremented from 100 ms to 95 ms timeslice, that is very marginally less timeslice, whereas a process that is decremented from 10 to 5 ms loses half of its running time!

timer ticks limit the possible values that timeslices can be assigned to (this varies accross systems, too)

adding priority to newly woken process improves interaction but allows sleep/wake use cases to game the schedulr

fundamental issue is that absolute timeslices yield a constant switching rate but variable fairness

CFS yields constant fairness but a variable switching rate

perfect multitasking would assign each process 1/n time, with each task running simultaneously
	impractical due to overhead of switching costs not to mention impossible

CFS doesn't assign timeslices, instead it weights a process according to its nice value

each process then runs for a "timeslice" proportional to its weight divided by the total weight of all runnable threads

CFS sets a target for its approximation of the infinitely small scheduling duration in perfect multitasking
    called targeted latency

default minimum timeslice is 1 ms, called minimum granularity

in this scheme, absolute nice values no longer affect scheduling decisions

CFS can be unfair if the proportion of processes becomes large enough that any given process recieves a proportion of less than 1, for instance if there were 20 processes with target latency of >20, However, the system was designed with this tradeoff explicitly acknowledged.

four components of CFS implementation will be discussed
     time accounting
     process selection
     the scheduler entry point
     sleeping and waking up

schedule entity structure: struct sched_entity

the scheduler entity is embedded in the task_struct (process descriptor) under name se

virtual runtime - actual runtime normalized by number of runnable processes -- nanoseconds
	choice of nanoseconds allows decoupling from processor tick
	helps approximate perfect multiprocessing



location of kernel/sched_fair.c has changed to kernel/sched/fair.c

in update_curr, it looks like the work is no longer being done by __update_cur
__update_cur used to cal calc_delta_fair, which is now called from update_cur, but the math is completely
different now. suffice it to say that niceness is exponential, and thats all for now

update_curr() is called by the system timer or whenever a process blocks or becomes unrunable

this way, vruntime is an accurate measure of the runtime of a given process and an indicator of what process should run next

CFS simply picks the process with the smallest vruntime

CFS uses a red-black tree to manage and find processes with minimum run time

cfs_rq stands for Completely Fair Scheduler Run Queue

the leftmost node (min vruntime) is cached for O(1) lookups
    if this cached node is empty, there are no runable processes, and the scheduler schedules the idle task

enqueue_entity basically updates vruntime of se, calls update_curr on the currently running entity, does accounting, and then inserts the entity into the cfs_rq. The __enqueue_entry procedure is basically a vanilla RB insert but it caches the leftmost entry.

when a process blocks (becomes unrunnable) or terminates (ceases to exist), we remove the process from the cfs_rq.

same thing: call update_curr, which will track how long the object was run in real-time, as well as vruntime (fair_delta)
dequeue_entity_load_avg: remove entities load average from the child load_average of rfs_rq
update_stats_dequeue: if the task waiting is being dequeued, mark the end of the wait period
CONFIG_SCHEDSTATS: can't locate where it is defined
cfs_runqueue_of: it looks like there are multiple cfs_rq objects, this returns the cfs_rq of an se
clear_buddies: depending on whether cfs_rq->(last, next, or skip) = se, iterate up the hierarchy, get the cfs_rq of the se, and if se== last, clear it
__dequeue_entity: basically an rb_delete, that if the deleted se is leftmost, make leftmost rb_next
update_min_vruntime: sets the vruntime of cfs_rq->vruntime to the minimum runtime

it appears that runqueues are separated by cpu

update_cfs_shares updates the total shares that a taskgroup recieves

__schedule:

pick_next_task iterates through each scheduler class from highest priority classes down and picks the highest priority process in that class, if it has a process.
	       so if the scheduler class is cfs, then we pick the leftmost value in the rbtree of fair procs

a sleeping task is ALWAYS waiting on a particular event

common reasons for sleeping:
       waiting on IO
       waiting for a contended semaphore
       other hardware event

behavior is the same regardless of the case:
	 task marks itself as sleeping, puts itself on waitqueue, removes itself from rb runnable tree, calls chedule

waking up is opposite:
       marks itself as runnable, removed from waitqueue, adds itself to runnable rbtree

two states are associated with sleeping, as before:
    TASK_INTERRUPTABLE
    TASK_UNINTERRUPTABLE

waitqueue is a list created statically through DECLARE_WAITQUEUE() or dynamically through init_waitqueue_head, which is a list wake_queue_head_t
processes put themselves on this list and mark themselves as not runnable

how to properly sleep:
    create a wait queue entry -- DEFINE_WAIT(wait);
    add_wait_queue(q, &wait);
    loop with wake up condition as the predicate
    prepare_to_wait(&q, &wait, *DESIRED_STATE*);
    if (signal_pending(current))
       handle signal
    schedule() -- END OF LOOP
    
    finish_wait

I am going to try taking notes within functions to see if it helps

from fs/notify/inotify/inotify_user.c

static ssize_t inotify_read(struct file *file, char __user *buf,
			    size_t count, loff_t *pos)
{
	
	struct fsnotify_group *group;
	struct fsnotify_event *kevent;
	char __user *start;
	int ret;
	DEFINE_WAIT(wait);

	start = buf;
	group = file->private_data;

	while (1) {
		prepare_to_wait(&group->notification_waitq, &wait, TASK_INTERRUPTIBLE);

		mutex_lock(&group->notification_mutex);
		kevent = get_one_event(group, count);
		mutex_unlock(&group->notification_mutex);

		pr_debug("%s: group=%p kevent=%p\n", __func__, group, kevent);

		if (kevent) {
			ret = PTR_ERR(kevent);
			if (IS_ERR(kevent))
				break;
			ret = copy_event_to_user(group, kevent, buf);
			fsnotify_destroy_event(group, kevent);
			if (ret < 0)
				break;
			buf += ret;
			count -= ret;
			continue;
		}

		ret = -EAGAIN;
		if (file->f_flags & O_NONBLOCK)
			break;
		ret = -ERESTARTSYS;
		if (signal_pending(current))
			break;

		if (start != buf)
			break;

		schedule();
	}

	finish_wait(&group->notification_waitq, &wait);
	if (start != buf && ret != -EFAULT)
		ret = buf - start;
	return ret;
}

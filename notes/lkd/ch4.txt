
scheduler divides finite resources of processing time between processes -- basis of multitasking operating system.

linux uses preemptive multitasking

time a process runs is usually predetermined, and is called a timeslice. -- linux does not employ timeslices, per se

the new scheduler of linux 2.5 was called O(1) scheduler
    great for large server loads
    shortcomings - interactive processes
    poor on desktops with interaction

Rotating Staircase Deadline scheduler introducted fair scheduling

Linux calls its scheduler the Completely Fair Scheduler -- CFS

policy is the behavior of a scheduler, and determines what runs when
       determines overall feel of a system
       responsible for optimally used processor time

processes are either:
	  I/O bound -- spends majority of time waiting for IO -- IO meaning any blockable resource
	      	       such as keyboard input, mouse, network IO, disk IO
	  processor bound -- spend much time executing code
	  	    	     tend to run until preempted
			     e.g. ssh-keygen

X Window server is both IO and processor bound
	  	    
scheduler must satisfy two conflicting goals:
	  low latency
	  high throughput

Linux favors IO bound, but is creative enough to not neglect processor bound processes

nice value - higher means lower priority
     control over proportion of the timeslice -- see nice values with ps -el

real-time priority -- default ranges from 0 to 99 higher is more priority

nice and real-time are in disjoin value spaces, with real-time having a higher priority than normal processes
     seen with ps -eo state,uid,ppid,rtprio,time,comm

timeslice -- how long a task can run until interrupted

too short of a timeslice is suboptimal because of larger switching overhead

too long and IO bound processes dont need the extra timeslice
processor bound processes want a long timeslice -- keeps cache "hot"

CFS assigns a proportion of the processor, which is a function of the load of the system

further affected by each processes nice value, which acts a weight, changing the proportion each process recieves

when a process enters an eligible state, whether or not it is run is a function of the proportion of the processor the newly runnable process has already consumed more of a proportion than the currently running process.

CFS is modular, allowing different algorithms to schedule different types of processes.
    called scheduler classes
    base scheduler iterates scheduler classes from highest to lowest, highest class that has a runnable task wins

DISCUSSION OF TRADITIONAL UNIX SCHEDULERS

mapping nice values into timeslices requires a decision about what absolute timeslice to allot each nice value

with high priority being assigned higher time slices, the proportion of processor time is totally dependant on what processes currently exist. Furthermore, since many high priority processes are IO bound, this is actually the opposite behavior of desired, becuase these high priority tasks only need a short running time, while "nice" processes are often processor bound, and prefer longer timeslices.


problem 2 -- if a process is decremented from 100 ms to 95 ms timeslice, that is very marginally less timeslice, whereas a process that is decremented from 10 to 5 ms loses half of its running time!

timer ticks limit the possible values that timeslices can be assigned to (this varies accross systems, too)

adding priority to newly woken process improves interaction but allows sleep/wake use cases to game the schedulr

fundamental issue is that absolute timeslices yield a constant switching rate but variable fairness

CFS yields constant fairness but a variable switching rate

perfect multitasking would assign each process 1/n time, with each task running simultaneously
	impractical due to overhead of switching costs not to mention impossible

CFS doesn't assign timeslices, instead it weights a process according to its nice value

each process then runs for a "timeslice" proportional to its weight divided by the total weight of all runnable threads

CFS sets a target for its approximation of the infinitely small scheduling duration in perfect multitasking
    called targeted latency

default minimum timeslice is 1 ms, called minimum granularity

in this scheme, absolute nice values no longer affect scheduling decisions

CFS can be unfair if the proportion of processes becomes large enough that any given process recieves a proportion of less than 1, for instance if there were 20 processes with target latency of >20, However, the system was designed with this tradeoff explicitly acknowledged.

four components of CFS implementation will be discussed
     time accounting
     process selection
     the scheduler entry point
     sleeping and waking up

schedule entity structure: struct sched_entity

the scheduler entity is embedded in the task_struct (process descriptor) under name se

virtual runtime - actual runtime normalized by number of runnable processes -- nanoseconds
	choice of nanoseconds allows decoupling from processor tick
	helps approximate perfect multiprocessing



location of kernel/sched_fair.c has changed to kernel/sched/fair.c

in update_curr, it looks like the work is no longer being done by __update_cur
__update_cur used to cal calc_delta_fair, which is now called from update_cur, but the math is completely
different now. suffice it to say that niceness is exponential, and thats all for now

update_curr() is called by the system timer or whenever a process blocks or becomes unrunable

this way, vruntime is an accurate measure of the runtime of a given process and an indicator of what process should run next

CFS simply picks the process with the smallest vruntime

CFS uses a red-black tree to manage and find processes with minimum run time

cfs_rq stands for Completely Fair Scheduler Run Queue

the leftmost node (min vruntime) is cached for O(1) lookups
    if this cached node is empty, there are no runable processes, and the scheduler schedules the idle task

enqueue_entity basically updates vruntime of se, calls update_curr on the currently running entity, does accounting, and then inserts the entity into the cfs_rq. The __enqueue_entry procedure is basically a vanilla RB insert but it caches the leftmost entry.

when a process blocks (becomes unrunnable) or terminates (ceases to exist), we remove the process from the cfs_rq.

same thing: call update_curr, which will track how long the object was run in real-time, as well as vruntime (fair_delta)
dequeue_entity_load_avg: remove entities load average from the child load_average of rfs_rq
update_stats_dequeue: if the task waiting is being dequeued, mark the end of the wait period
CONFIG_SCHEDSTATS: can't locate where it is defined
cfs_runqueue_of: it looks like there are multiple cfs_rq objects, this returns the cfs_rq of an se
clear_buddies: depending on whether cfs_rq->(last, next, or skip) = se, iterate up the hierarchy, get the cfs_rq of the se, and if se== last, clear it
__dequeue_entity: basically an rb_delete, that if the deleted se is leftmost, make leftmost rb_next
update_min_vruntime: sets the vruntime of cfs_rq->vruntime to the minimum runtime

it appears that runqueues are separated by cpu

update_cfs_shares updates the total shares that a taskgroup recieves

__schedule:

pick_next_task iterates through each scheduler class from highest priority classes down and picks the highest priority process in that class, if it has a process.
	       so if the scheduler class is cfs, then we pick the leftmost value in the rbtree of fair procs

a sleeping task is ALWAYS waiting on a particular event

common reasons for sleeping:
       waiting on IO
       waiting for a contended semaphore
       other hardware event

behavior is the same regardless of the case:
	 task marks itself as sleeping, puts itself on waitqueue, removes itself from rb runnable tree, calls chedule

waking up is opposite:
       marks itself as runnable, removed from waitqueue, adds itself to runnable rbtree

two states are associated with sleeping, as before:
    TASK_INTERRUPTABLE
    TASK_UNINTERRUPTABLE

waitqueue is a list created statically through DECLARE_WAITQUEUE() or dynamically through init_waitqueue_head, which is a list wake_queue_head_t
processes put themselves on this list and mark themselves as not runnable

how to properly sleep:
    create a wait queue entry -- DEFINE_WAIT(wait);
    add_wait_queue(q, &wait);
    loop with wake up condition as the predicate
    prepare_to_wait(&q, &wait, *DESIRED_STATE*);
    if (signal_pending(current))
       handle signal
    schedule() -- END OF LOOP
    
    finish_wait

I am going to try taking notes within functions to see if it helps

from fs/notify/inotify/inotify_user.c

static ssize_t inotify_read(struct file *file, char __user *buf,
			    size_t count, loff_t *pos)
{
	
	struct fsnotify_group *group;
	struct fsnotify_event *kevent;
	char __user *start;
	int ret;
	// create a wait queue entry
	DEFINE_WAIT(wait);

	start = buf;
	group = file->private_data;
	// since the condition check is complicated, we just quit through break statements
	while (1) {
	      	  // looks like they skip add_to_queue and allow prepare_to_wait to do the work
		  // They made the state interruptible
		prepare_to_wait(&group->notification_waitq, &wait, TASK_INTERRUPTIBLE);

		mutex_lock(&group->notification_mutex);
		// get an event for the notification waitgroup
		kevent = get_one_event(group, count);
		mutex_unlock(&group->notification_mutex);

		pr_debug("%s: group=%p kevent=%p\n", __func__, group, kevent);

		if (kevent) {
			ret = PTR_ERR(kevent);
			if (IS_ERR(kevent))
				break;
			ret = copy_event_to_user(group, kevent, buf);
			fsnotify_destroy_event(group, kevent);
			if (ret < 0)
				break;
			// this is interesting, shifts the buffer
			buf += ret;
			count -= ret;
			continue;
		}
		// we don't make it to here if there was an event
		ret = -EAGAIN;
		if (file->f_flags & O_NONBLOCK)
			break;
		ret = -ERESTARTSYS;

		// not sure here
		if (signal_pending(current))
			break;
		// if the buffer changed at all, we are done
		if (start != buf)
			break;
		// go to sleep
		schedule();
	}
	// set status to TASK_RUNNING, removes group from wait queue
	finish_wait(&group->notification_waitq, &wait);
	if (start != buf && ret != -EFAULT)
		ret = buf - start;
	return ret;
}

wake_up wakes all tasks on a given wait queue

calls try_to_wake_up, which sets task to task running, adds to rbtree, and sets need_resched if task is higher priority than current task

the code that causes the event typically calls wake_up, for instance the vfs usually calls wake_up when data arrives from hard disk

schedule itself calls deactivate_task to remove a task from the runqueue

so there are two cases:

first, the task could be awoken, in which case the task is set to task running and the task executes its signal handler to determine if the event was the one being waited for. if it wasn't the task goes back to sleep

if it was, the system wakes up all tasks on the waitqueue with try to wake up, which sets tasks to runnable, adds the task to the rbtree, tags need_resched if it applies, and then removes the task from the waitqueue

why does __wake_up need a spinlock?
    it looks like a spinlock disables preemption, in __raw_spinlock_irqsave() I'll come back to this

it appears that each sleeping task in the wait_queue stores a function to wake up with, which could possible normally be try_to_wake_up

context switching occurs whenever a new process has been selected to run. it does two basic jobs
	switch_mm to switch to the virtual memory mapping for the new process
	switch_to, which switches the processor state the new process

preemption is basically implemented through the needs_resched flag of a process.
	   this flag is set by scheduler_tick, or try_to_wake_up if a process has higher priority than the running task

functions for manipulating needs_resched
	  set_tsk_need_resched
	  clear_tsk_need_resched
	  need_resched

it is much faster to access this value as a single bit within a flag variable of the thread_info of the currently running process than globally, because the thread_info struct is likely to be cache hot

interestingly, context_switch is an inline function

prepare_task_switch appears to just be accounting functions

context_switch calls arch_start_context_switch, which does something for paravirt, and I'm not going to get side-tracked

if the next process does not have an mm struct, the next->active struct is set to the oldmm. Then the oldmm mmcount is inc'ed and we enter_lazy_tlb

otherwise, switch_mm to new task's mm

if CONFIG_SMP
prepare processor for context switch
tlbstate set to TLBSTATE_OK, but then switched???
the tlbstate is set to the next process

set the cpu mask
load cr3 with the pgd for next process
flush tlb
load the new Local Descriptor Table if prev->context.ldt != next->context.ldt

BUG_ON is basically an assertion that printk's and then panic()s

if prev == next
if CONFIG_SMP, the kernel will panic if the current cpu's tlbstate doesn't match next, aka that doesnt make sense at all
since irq is disabled during schedule, and SMP is enabled, we know that this case occurs when the cpu was in tlb lazy mode, so the context switch implies that we must flush tlb, because we are running a new user space process?? CHECK THESE ASSUMPTIONS WHEN YOU KNOW MORE

switch_to loads all registers with saved values. Note that this is architecture specific

anatomy of switch_to:
{
switch_to is a macro, which dumps the following asm instructions:
	  immediately pushes flags, then base-pointer onto stack
	  saves old esp, loads new esp
	  saves old eip (one forward), loads new eip
	  jumps to __switch_to
	  after return from switch_to
	  after all of those function calls, restore ebp and eflags, and all registers
	  are officially in the correct state for next task
	  it looks like the code after the 1: tag replaces the use of a ret to the caller of switch_to
	  

__switch_to:
	prepare the floating-point unit for switch
	load sp0 with tss of next task
	save gs segment in prev->gs
	load per-thread-Thread Local Storage descriptor (found in GDT)
	restore IOPL
	there could be a discrepancy between the preempt count if the current thread is being preempted
	PREEMPT active, so we update the preempt count of the cpu to preempt count of next
	do something with IO bitmaps and debug registers in __switch_to_xtra
	arch_end_context_switch appears to be related to paravirt
	write the location of the kernel stack onto current cpu
	restore gs segment descriptor if needed
	finish loading the floating point unit (restore maybe?)
	write current task to be next_p (which is a task struct)
}

IOPL - IO privilege level

side note on Task state segment (TSS)
special structure in x86, used by kernel for task management
stores the following information
       register state
       IO port permissions
       Inner-level stack pointers
       Previous TSS link
and now we have it...The Task register holds a segment selector that points to a valid TSS segment descriptor, which resides in the GDT (cannot reside in LDT)

tss_struct in linux:
	   x86_tss, op_bitmap, emergency kernel stacks


	   
	




user preemption occurs when the kernel is about to return to the user-space and need_resched flag is on
     it is safe to either continue with current task, or switch

user preemption can occur when returning from
     user-space system call
     user-space interrupt handler

in nonpremptive kernels, kernel code runs until completion
Linux on the other hand is fully preemptive - any task is preemptible so long as the kernel is in a safe state

locks designate segments of code that are unsafe for preemption

prempt_count is saved in each processes thread_info struct
	     starts at 0 inc for each lock, dec for each release
	     when 0, preemptible

after an interrupt, if needs_resched is set and preempt_count is 0, then the scheduler is safe to choose a task of higher priority and perform a context switch. However, if prempt_count is greater than 0, then the lock is held and the current task continues to execute until its locks are released.
      unlock code checks if need_resched is set

kernel preemption can occur in these cases:
       when an interrupt handler exits before returning to user space
       when kernel code becomes preemptible again
       if a task in the kernel explicity calls schedule
       if a task in the kernel blocks (which results in a call to schedule)


most system calls that interact with process priority are setters or getters for variables saved within a tasks task_struct

the nice() system call sets static_prio and prio values in the task struct

affinity is what processors a task is allowed to run on
	 in linux hard affinity is enforced, with a bitmask cpus_allowed
	 
process inherits its parents affinity mask

when a process changes its affinity, the kernel uses migration threads to push the task to a legal processor

the load balancer pulls tasks only to an allowed processor

sched_yield removes task from active array and puts it in the expired array

real_time tasks never expire so they are an exception and are moved to the end of the priority list

kernel code can use the yield function, which checks to make sure state is TASK_RUNNING, and then calls sched_yield



kernel schedules threads, not processes

linux does not differentiate between threads and processes, it just defines a special type LWP I presume

processes provide two kinds of virtualization:
	  illusion that processor monopolises both the CPU (Scheduling) and Memory (Memory Management)

process is created by fork()
	fork() returns twice, once in parent and once in child
	exec() to start a new child program

fork() is implemented by clone()

program exits by the exit() system call -- this terminates the process and frees resources

parent can wait for child with wait4() or waitpid()

processes are referred to as tasks in linux interals

circular task lists holds all processes

each element is a process descriptor

slab allocated provides object reuse and cache coloring

thread_info lives at the bottom of the stack and contains:
task, exec_domain, flags, status, cpu, and preempt_count, 
addr_limit, restart_block, sysenter_return, uaccess_err

an opaque type is a data type whose physical representation is unknown or irrelevant

pid_t is the pid process type

the size of pid_max can be set at /proc/sys/kernel/pid_max

inside the kernel all references to processes are typically through pointers to task_struct

the current macro returns the currently executing task

x86 must use kernel stack to locate thread_info, which can access its task attribute to return current

PPC architectures have a register that holds a reference to current task struct

from sched.h

#define TASK_RUNNING		0
#define TASK_INTERRUPTIBLE	1
#define TASK_UNINTERRUPTIBLE	2
#define __TASK_STOPPED		4
#define __TASK_TRACED		8
/* in tsk->exit_state */
#define EXIT_DEAD		16
#define EXIT_ZOMBIE		32
#define EXIT_TRACE		(EXIT_ZOMBIE | EXIT_DEAD)
/* in tsk->state again */
#define TASK_DEAD		64
#define TASK_WAKEKILL		128
#define TASK_WAKING		256
#define TASK_PARKED		512
#define TASK_STATE_MAX		1024


TASK_RUNNING - either running or on a run-queue
TASK_INTERRUPTIBLE -- process is sleeping
TASK_UNINTERRUPTIBLE -- sleeping but won't wake up on a signal
__TASK_TRACED -- being traced (debugged usually)
__TASK_STOPPED -- Not running and ineligible to run

schedule() starts running a task with context_switch() either the task exits, or goes to sleep (TASK_INTERRUTIBLE OR TASK_UNINTERRUPTIBLE).

change a task state through set_task_state(task, state);
on SMP provides a memory barrier

set_current_state(state) is a synonym without the task arg. Locks are generated through in arch specific code,
in x86, by macros that dump assembly within asm volatile() assembly functions. These locks are referred to as "memory barriers"

normal program execution -- user space
system call or exception -- kernel space, kernel is executing on behalf of process "in process context"

ALL ACCESS TO KERNEL-SPACE IS THROUGH SYSTEM CALLS AND EXCEPTION HANDLERS

all processes are children of init, which has a pid of 1

list_for_each takes a start point and creates the for (...) part of a foor loop.
list_entry(ptr, type, member); returns container_of the pointer

task_structs store lists of children, pointers to parent and real_parent (figure out the difference) and sibling, also group leader for thread groups

get next task:
task_entry(task->tasks.next, struct task_struct, tasks); == next_task(task)
get prev task:
task_entry(task->tasks.prev, struct task_struct, tasks); == prev_task(task)

for_each_process(task) iterates entire task list

after fork(), child and parent are identical EXCEPT FOR:
      pid, ppid(parent's process id), and inherited pending signals and other resources and statistics

Copy-On-Write -- after fork() processes share resources until a write, when they each recieve their own unique copy

the only overhead created by fork() is the duplication of parent's page tables and creation of a process descriptor

fork() vfork() and __clone() call clone(), which calls do_fork(), which does interesting work in copy_process

dup_task_struct creates a new stack, thread_info structure, and a new task_struct

since thread_groups share signals, a detached thread can only be copied as a member of the thread group

process wont be copied if the process being copied has shared virtual memory and shared signal handlers and blocked signals

global and container inits are not allowed to create siblings

start on page 32 LKD Forking && kernel/fork.c

capable checks whether superuser priviledges flag is true

I don't see any reference to copy_flags as referenced by love, might be a different function

so basically in copy process, the struct is copied, some process specific statistics are cleared, common files, signal handlers, address space, and others are transferred, the process is put to sleep in an uninterruptible state, a new pid is allocated, etc.


in vfork, the parent waits for the child process to either exit or call exec
child executes as a single thread in parent adress space
      only benefit is that the parent page tables are not copied, otherwise sort of redundant due to copy-on-write

in Linux, threads are really just processes that share resources. They are called lightweight processes. Other operating systems explicitly implement threads.

calls to do_fork by differing functions:

fork:
clone(SIGCHLD, 0);

vfork:
clone(CLONE_VFORK | CLONE_VM | SIGCHLD, 0);

thread:
clone(CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SIGHAND, 0);

aka address space, filesystem resources, file descriptors, and signal handlers are shared.

A full list of the clone flags is in include/linux/sched.h

kernel threads do not have their own address spaces (mm_struct attribute is NULL)
       operate only in kernel space, no context switching
       still schedulable and preemptable

kernel threads handle flush tasks and ksoftirqd

they can be seen with ps -ef

created on system boot by other kernel threads

forking of all new kernel threads is handled by kthreadds

in kthreadd : what does "for (;;)" do?

a kthread must be created and then started with wake_up_process

kthread_stop(struct task_struct *k); stops a kthread with the task_struct of the given address

do_exit

do_exit releases all of the system resources a process has been using, this includes filesystem resources, file descriptors, releases the process from semaphore queues, releases timers, releases memory usage (with the exception of task_struct, thread_info, and the kernel stack, which happens later), sets exit code to the code provided by exit, sends exit_notify to parent, children, sets exit_state to EXIT_ZOMBIE, and calls schedule(), which, since the task is no longer schedulable, means the process is dead.

RCU stand for Read-copy update, which can sync multiple readers with one updater.

it uses a publish-subscribe mechanism, which ensures ordering. rcu_assign_pointer protects writes, while rcu_read_lock, rcu_dereference, and rcu_read_unlock aid in reads

rcu_read_lock and rcu_read_unlock specify read-critical sections, ensuring that readers will see any assigments that occur before rcu_assign_pointer

usually rcu_assign_pointer and rcu_dereference are embedded in higher-level constructs like list_add_rcu, which allow updating and reading of doubly linked lists simultaneously

calling release_task calls __exit_signal, which informs all of the relevant processes that this thread is going to be released. Then unhash_process releases its pid, and if the group is dead if frees the group's pids
	removes process from task list

___exit_signal also releases statistics and bookkeeping

if this thread is the last in a thread group and the group leader is a zombie, then the group leader's parent is notified

the process is finally released. If this was the last functioning thread and the parent ignores SIGCHILD, then we the thread becomes the leader and repeats with a goto

all children on thread exit are reparented to either another thread group member, or to the init process. This occurs during do_exit, which calls exit_notify, which calls find_new_reaper


find_new_reaper returns a suitable replacement namespace to become the new reaper

exit_notify does two things: - makes init inherit all children, and if any processes are orphaned from exiting, send them SIGHUP and SIGCONT

in forget_original_parent
each child process recieves the new reaper as its parent, and real-parent if the thread in question has father as its parent

then all processes that are dead are released.

init routinely calls wait(), cleaning up any zombies it inherits through this process

scheduler divides finite resources of processing time between processes -- basis of multitasking operating system.

linux uses preemptive multitasking

time a process runs is usually predetermined, and is called a timeslice. -- linux does not employ timeslices, per se

the new scheduler of linux 2.5 was called O(1) scheduler
    great for large server loads
    shortcomings - interactive processes
    poor on desktops with interaction

Rotating Staircase Deadline scheduler introducted fair scheduling

Linux calls its scheduler the Completely Fair Scheduler -- CFS

policy is the behavior of a scheduler, and determines what runs when
       determines overall feel of a system
       responsible for optimally used processor time

processes are either:
	  I/O bound -- spends majority of time waiting for IO -- IO meaning any blockable resource
	      	       such as keyboard input, mouse, network IO, disk IO
	  processor bound -- spend much time executing code
	  	    	     tend to run until preempted
			     e.g. ssh-keygen

X Window server is both IO and processor bound
	  	    
scheduler must satisfy two conflicting goals:
	  low latency
	  high throughput

Linux favors IO bound, but is creative enough to not neglect processor bound processes

nice value - higher means lower priority
     control over proportion of the timeslice -- see nice values with ps -el

real-time priority -- default ranges from 0 to 99 higher is more priority

nice and real-time are in disjoin value spaces, with real-time having a higher priority than normal processes
     seen with ps -eo state,uid,ppid,rtprio,time,comm

timeslice -- how long a task can run until interrupted

too short of a timeslice is suboptimal because of larger switching overhead

too long and IO bound processes dont need the extra timeslice
processor bound processes want a long timeslice -- keeps cache "hot"

CFS assigns a proportion of the processor, which is a function of the load of the system

further affected by each processes nice value, which acts a weight, changing the proportion each process recieves

when a process enters an eligible state, whether or not it is run is a function of the proportion of the processor the newly runnable process has already consumed more of a proportion than the currently running process.

CFS is modular, allowing different algorithms to schedule different types of processes.
    called scheduler classes
    base scheduler iterates scheduler classes from highest to lowest, highest class that has a runnable task wins

DISCUSSION OF TRADITIONAL UNIX SCHEDULERS

mapping nice values into timeslices requires a decision about what absolute timeslice to allot each nice value

with high priority being assigned higher time slices, the proportion of processor time is totally dependant on what processes currently exist. Furthermore, since many high priority processes are IO bound, this is actually the opposite behavior of desired, becuase these high priority tasks only need a short running time, while "nice" processes are often processor bound, and prefer longer timeslices.

















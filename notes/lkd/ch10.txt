KERNEL SYNCHRONIZATION METHODS

atomic operations are the foundation of sync methods

it is not possible for two atomic operations to occur on the same variable concurrently

two sets of interfaces for atomic operations
    integers
    individual bits
implemented on every arch that Linux supports

on archs where cpu doesnt support the necessary atomic operation, the memory bus can be locked

atomic data operations occur on a special data type
       atomic_t
       ensures that atomic operations are only used with special types
       ensures that datatypes are not passed to nonatomic functions
       ensures compiler doesn't optimize
       hides architecture specific differences

it looks like atomic_t no longer has the volatile prefix

usually an unsigned 32-bit integer

declarations are in <asm/atomic.h>

most of the operations are pretty straightforward

to convert an atomic_t to an int:
   atomic_read(&v)

atomic_dec_and_test returns true if the result is 0 else false

on most architectures, a read as inherently atomic so the implementation is usually just a macro or inline

atomicity does not imply ordering
	  ordering is enforced via barrier operations

on 64 bit archs, use atomic64_t
   basically all of the same functionality as atomic_t

for portability, almost always use atomic_t

BITWISE OPERATIONS
defined in <asm/bitops.h>n

unlike atomic_t, there is no corresponding type for atomic bit operations, because they take generic pointer

atomic bit instructions can be mixed with normal C

everything you might expect, set bit, clear bit, flip bit, test and whatever bit
	   all start from a generic address


nonatomic versions also exist
	  usually prefixed with double underscores

atomically find first set bit or unset bit
	   int find_first_bit(unsigned long *addr, unsigned int size)
	   int find_first_zero_bit(unsigned long *addr, unsigned int size)

SPIN LOCKS

a spin lock is a lock that can be held by at most one thread
  when a thread is already locked and another thread attempts access, the thread is said to be contended
  the busy thread spins

spin locks are lightweight locks that are only meant to be held for short periods of time

must be weighed with alternative -- sleeping (semaphores). This has the overhead of a context switch

spinlocks are arch-dependant, implemented in assembly

looks like a spinlock is a struct which has an attribute .raw_lock, .dep_map

basic use:

DEFINE_SPINLOCK(mr_lock);
spin_lock(&mr_lock);
spin_unlock(&mr_lock);

on uniprocessing machines, locks compile away, but still signify regions where preemption is disabled

spinlocks are not recursive, so do not try to acquire a spinlock that is held by the same thread
	  this will lead to deadlock

interrupts must be disabled before acquiring locks in many cases
	   otherwise, an interrupt handler could wait on an already held lock, but it will never release


interface to do this is

DEFINE_SPINLOCK(mr_lock);
unsigned long flags;

spin_lock_irqsave(&mr_lock, flags);
spin_unlock_irqrestore(&mr_lock, flags);

when sure that interrupts are already enabled
NOT RECOMMENDED
spin_lock_irq(&mr_lock);
/* critical region */
spin_unlock_irq(&mr_lock);

when testing code with spin locks
     CONFIG_DEBUG_SPINLOCK
     CONFIG_DEBUG_LOCK_ALLOC

spin_lock_init() for dynamic allocation

spin_trylock()
	zero if locked nonzero otherwise

spin_is_locked()
	0 if "open", nonzero if given lock is currently acquired

the previous two uses are uncommon, as usually code should attempt to acquire a lock, or be called while lock
    is already held

ok, this is starting to get complicated

a bottom half may interrupt process context, because of this, when in process context, data must be protected
  with both a lock and disabling of bottom halves

likewise, BH can be preempted by an interrupt, so data must be protected during BH with lock and disable irc

since tasklets of same type can't run simultaneously, only necessary to protect shared data
      between diff tasklets


multiple readers are ok as long as write isn't occur in some contexts
	 hence reader-writer spin locks

conversely, writer lock can be held by at most one writer without concurrent readers

sometimes called shared/exclusive or concurrent/exclusive locks

initialized via

DEFINE_RWLOCK(mr_rwlock);

for reading:

read_lock(&mr_rwlock);
/* critical path */
read_unlock(&mr_unlock);

for writing:

write_lock(&mr_rwlock);
/* critical path */
write_unlock(&mr_rwlock);

cannot upgrade a read lock to a write lock:

read_lock(&mr_rwlock);
write_lock(&mr_rwlock);

THIS WILL DEADLOCK
     normally if mixing between reads and writes is needed, just use reg spin lock

rwlocks favor readers over writers

semaphore
	sleeping lock
	much greater overhead than spin lock
	well suited to long running locks
	must be obtained only in process context
	it is possible to sleep while holding a semaphore without deadlocking
	cannot acquire a semaphore while holding a spin lock, because sleep may be needed
	semaphores are preemptible unlike spinlocks -- do not adversely affect scheduling latency
	
when task tries to acquire unavailable semaphore, the semaphore adds task to a wait queue and puts the task
     to sleep. When the semaphore becomes available, it wakes a sleeping task on the queue

number of allowed users for a semaphore can be set at declaration

binary semaphore
       one user, also known as a mutex
       
counting semaphore
	 arbitary threads may access, also set at declaration

most of the time in the kernel we want mutexes

two methods:
    down() - acquire -- if the count is negative, the process sleeps, 0 or greater it can enter crit region
    	   decrements the count
    up() - release after crit region, increments the count

semaphore implementation is architecture dependant (gee, isn't everything?)
	  defined in <asm/semaphore.h>

looks like the mutex specific macros may not be around anymore, instead:
      DEFINE_SEMAPHORE(name); seems to serve that purpose

sema_init(struct semaphore *sem, int val); for control over count

down_interruptible() will attempt to acquire the given semaphore, and go TASK_INTERRUPTIBLE if not
		     returns -EINTR if task was awakened during slumber

down places task in status TASK_UNINTERRUPTIBLE

down_interruptible() is more commonly used

down_trylock() will try the semaphore
	       nonzero if already held, but the task doesn't sleep
	       zero if not already held

just like with spinlocks, there are reader-writer semaphores

static declaration:
static DECLARE_RWSEM(name);

dynamic:
init_rwsem(struct rw_semaphore *sem);

the are all mutexes, just only for writers

down_read(&sem);
up_read(&sem);
down_write(&sem);
up_write(&sem);

also down_write_trylock() and down_read_trylock()
     opposite return values than with normal semaphroes, nonzero if acquired, zero if not
     	      WHY??? - this is crazy

downgrade_write() -- allows upgrade of reader to writer spinlock

now there as something called an actual mutex that implements a lighter weight mutual exclusion that sem

declare:
DEFINE_MUTEX(name)

dynamic:
mutex_init(&mutex);

mutex_lock(&mutex);
mutex_unlock(&mutex);

simpler interface -- more efficient
	only one task can hold a mutex at a time
	whoever locked a mutex must unlock
	recursive lock and unlock not allowed
	process cannot exit while holding a mutex
	cannot be acquired by interrupt handler or bottom half even with mutex_trylock()
	managed only via official API

benefit is that a special debugging mode is available to check for violations

prefer mutexes to semaphore unless mutex constraints do not allow
       start with mutex and go to semaphore if necessary

completion variables are very similar to semaphores
	   one task works while another waits for a signal through the completion variable
	   when task is done working, it wakes up other tasks through the completion variable
	   vfork() uses completions while parent waits for child to exec or exit

completions represented by a struct

initialize via:
DECLARE_COMPLETION(mr_comp);

dynamic:
init_completion()

wait with:
wait_for_completion()

complete() wakes up waiting tasks -- this function is not easy to find for some reason

BKL -- Big Kernel Lock
    can sleep while holding BKL
    	will be disabled when task is unscheduled, and enabled when task resumes
	recursive
	only in process context
	new users of BKL are forbidden -- slowly being phased out

still can be seen throughout kernel, thus important to understand

behaves like a spin lock with addition of previously listed properties

lock_kernel()
unlock_kernel()

kernel_locked()
	nonzero if lock currently held, otherwise zero

BKL disables kernel preemtion

seq locks
    simple mechanism for reading and writing shared data
    used when:
    	 data has a lot of readers
	 few writers
	 writers are favored and readers should never starve writers
	 simple data
    if count is odd, then a write_seqlock exists, even, no writers
    readers compare count at start and finish, if equal, no change

common use of seq locks is for reading jiffies
       jiffies are the number of clock ticks since the machine booted
       since 64 bit some machines cannot atomically read jiffies_64


sometimes data need not be locked, but preemption still poses a pseudo-concurrency risk

preempt_disable() -- increments preemption counter
preempt_enable() -- decrements preemption counter -- also services reschedules if counter == 0
preempt_disable_no_resched() -- obvious
preempt_count() -- returns preemption count

alternative solution to per-processor data:
	    use get_cpu() to disable preemption
	    put_cpu() to reenable


some processors (not x86) can reorder instructions for optimization reasons

compilers also reorder instructions for optimization

barriers prevent this behavior (through arch specific instructions usually)

rmb()
	ensures that no loads are reordered across the rmb() call
		no loads before rmb() will be moved to after
		no loads after rmb() will be moved to before

wmb()
	stores instead of loads, but otherwise the same as rmb

mb()
	read and write

read_barrier_depends()
	only for loads on which a subsequent load depends
	     all reads prior to barrier are guaranteed to complete before any reads after the barrier
	     	 that depend on reads prior to barrier

smp_rmb(), smp_wmb(), smb_mb(), smp_read_barrier_depends()
	   all potential optimizations, because on UP machines, only compiler barriers are needed

barrier() method prevents the compiler from optimizing loads across the call

compiler_barriers are lighter, but are not aware of potential for memory access outside of current context

this stuff is complex and will need further review

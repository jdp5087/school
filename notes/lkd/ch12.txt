MEMORY MANAGEMENT

memory is managed by the MMU -- makes the translation from virtual address to physical address

pages are generally 4KB on 32-bit archs, and 8KB on 64-bit
      thus with 4KB pages and 1GB memory, there are 262,144

struct page {
flags is unsigned long so 32 bits available
_count is the usage count of the page -- -1 when free for reallocation
       _count should be accessed with function page_count()
       page may be used by:
       	    page cache -- in which case the mapping field points to the address space object associated
	    private data -- pointed at by private
	    or as a mapping in a processes page table
virtual - void poitner to the page's virtual address
	NULL if highmem

];

the important part to understand is that the page table describes the physical memory of a page
    not the data contained therein
    important to note that due to swapping, the data underneath the page struct is subject to change

possible owners of a page are
	 user-space processes
	 dynamically allocated kernel data
	 static kernel data
	 page cache
	 more

not all pages can be treated identically
    some physical addresses cannot be used for certain tasks

pages are divided into zones

two shortcomings for hardware:
    some hardware devices can perform DMA (direct memory access) to only certain mem addresses
    some archs physically address more than they can virtually address
    	 some mem not mapped into kernel address space

four primary memory zones
     ZONE_DMA
     ZONE_DMA32
     ZONE_NORMAL -- regularly mapped pages
     ZONE_HIGHMEM

two other parts of the enum:
    ZONE_MOVABLE
    __MAX_NR_ZONES

devices on x86 that need special DMA zone:
	ISA -- can only access first 16 MB of physical memory

on 32-bit x86, (highmem is also arch specific) ZONE_HIGHMEM is above 896MB

the rest is low memory

on x86, ZONE_NORMAL is between 16MB and 896MB
   remember, we are still talking actual physical addresses

these zones are basically pools that tell the kernel where to look for pages under each category
      allocations can not cross boundaries

x86-64 can fully map 64-bits of memory, so it doesn't have highmem

each zone is represented by a struct zone

struct zone {
has a spinlock -- not pages, just the zone lock
watermark is an array that holds min, low and high watermarks for a zone
	  used to set benchmarsk for suitable per-zone memory

};

get page:
struct page * alloc_pages(gfp_t gfp_mask, unsigned int order);

get logical address of page:
void * page_address(struct page *page);

if struct isnt needed, get logical address of alloced page with:
unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order);

pages are continuous, so you can just figure out where pages are if you are given the starting point

get zeroed page (especially useful for allocing pages for user space)
unsigned long get_zeroed_age(unsigned int gfp_mask);

must handle errors during memory allocation, because if we use free_pages on memory we don't "own"
     corruption can occur
     best to attempt mem alloc early, that way unwind isn't as difficult

kmalloc is more general and not restricted to page-size chunks

with kmalloc, allocation is in byte-sized chunks

void * kmalloc(size_t size, gfp_t flags);
     notice the flags argument (not seen in userspace malloc)
     might round up to a page since lower level allocs are page-based
     never returns less than requested, in that case would return NULL

gfp_t is an unsigned in typedef that holds gfp flags
      gfp stands for get free page

gfp flags are broken down into three categories
    action modifiers -- how the kernel is supposed to allocate memory (sleep, retry, etc)
    zone modifiers -- where to get the memory from
    types -- premade combination of action and zone modifiers

apparently these flags are important, so here are their meanings

__GFP_WAIT -- allocator can sleep
__GFP_HIGH -- can use emergency pools
__GFP_IO -- can start disk IO
__GFP_FS -- can start filesystem IO
__GFP_COLD -- can use cache cold pages
__GFP_NOWARN -- don't print alloc failure warnings
__GFP_REPEAT -- can repeat (This can still fail eventually)
__GFP_NOFAIL -- indefinitely tries allocation
__GFP_NORETRY -- Don't try again
__GFP_NOMEMALLOC -- does no fall back on reserves
__GFP_HARDWALL -- "hardwall" cpuset boundaries
__GFP_RECLAIMABLE -- marks pages reclaimable
__GFP_COMP -- adds compound page metadata

zone modifiers
     __GFP_DMA -only from this zone
     __GFP_DMA32 -only from this zone
     __GFP_HIGHMEM - ZONE_HIGHMEM or ZONE_NORMAL

for funcs __get_free_pages() or kmalloc()
    cannot specify ZONE_HIGHMEM
    because it is possible that this would allocate pages not mapped in kernels virtual address space,
    	    thus it wouldn't have a logical address
    only alloc_pages() can take ZONE_HIGHMEM flag

type flags:

GFP_ATOMIC - high priority and doesnt sleep -- used in int handlers, BH's spinlocks
	   much less likely to succeed since blocking isn't possible
GFP_NOWAIT -- same as first except wont fall back on emergency mem pools
GFP_NOIO -- can block, but no disk IO -- used when already in block IO code
GFP_NOFS -- can block and initiate disk IO, but no FS IO -- used when already in filesystem code
GFP_KERNEL -- normal allocation, might block. used in process context when safe to sleep
	   likely to succeed
GFP_USER -- normal alloc, might block. used for user-space processes
GFP_HIGHUSER -- used for ZONE_HIGHMEM and might block
GFP_DMA -- used for ZONE_DMA -- usually used in combination with one of prev types

what to use when

process context, can sleep -- GFP_KERNEL
process context, cannot sleep -- GFP_ATOMIC or perform allocs before or after no sleep region
interrupt handler -- GFP_ATOMIC
Softirq -- GFP_ATOMIC
Tasklet -- GFP_ATOMIC
DMA, can sleep -- (GFP_DMA | GFP_KERNEL)
DMA, can't sleep -- (GFP_DMA | GFP_ATOMIC)

kfree() is pretty straightforward, just be sure to always balance kmallocs with kfrees()
	never kfree() a block of memory that wasn't successfully kmalloc()ed

vmalloc() is like kmalloc
	  BUT -- mem is only virtually contiguous, not necessarily physically contig
	  this is how user space malloc works
	  so physically non contiguous areas can have page tables fixed up to appear virtually contiguous
	     through mem-mapping

generally only hardware devices need physically contiguous areas
	  this revolves around DMA, they live on other side of MMU, and don't understand virt addresses

even though most kernel code could use vmalloc, it uses kmalloc for performance reasons

vmalloc can thrash TLB because it must map individual pages instead of regions as in kmalloc
	TLB is a hardware cache of the mapping from virt to phys addresses

so use vmalloc only when absolutely necessary like when large regions of memory are needed
   for instance when dynamically creating space for modules -- uses vmalloc()

vmalloc only needs size of allocation in bytes -- no flags
	might sleep
	
vfree is its foil
      also might sleep

free-lists
	already allocated data structures that are available

data structures can be used and then returned to free list
     problem is that kernel doesnt understand free lists, cant cut down on cache sizes during memory pressure

goals accomplished by slab allocator
      cache frequently used data structures (because they are allocated and freed often)
      store free lists contiguously
      improves performance during frequent alloc and frees
      awareness of object size, page size, and total cache size results in smarter allocator
      percpu allocation and frees dont need SMP lock
      if NUMA, can allocate from same node as requestor
      coloring prevents multiple objects from being mapped to the same cache line
      
many different caches, each holds a specific type of object
     for instance task_struct or struct inode
     
caches are then divided into slabs, physically contiguous pages
       typically only a single page

three states for each slab
      full, partial or empty

full means all objects are allocated

struct inode is allocated through slab from inode_cachep

during allocation, first try from partial slabs, then from empty
       if no empty exists, create a new one
       this reduces fragmentation

the slab allocator returns a pointer to an unused object
when done, the allocator marks the object as free

each cache is represented by a kmem_cache structure
     contains three lists
     	      slabs full
	      slabs partial
	      slabs empty

can't find definition of slab anywhere
      looks like kmem_cache hold many of the variables that slab held
     then inside kmem_cache there is kmem_cache_cpu, which is a percpu cache

slab allocates memeory for new slabs with __get_free_pages()
     doesn't this imply that unless all allocs occur at one time, then they won't be contiguous?
     

slab descriptors are allocated either outside the slab in a general cache or inside the cache itself in the beg    descriptor is stored inside the slab itself if it is small enough or if internal
     		slack space is sufficient to hold descriptor

WARNING -- THIS COULD BE OUT-OF-DATE
kernel swap daemon -- kswapd
       kernel thread -- no virtual mem, only physical mapping
       manages swaps and ensures that enough free pages exist for efficiency
       started by init, triggered by swap timer
       checks to see if free pages are above a certain threshold in both high and low memory

       three strategies for reducing number of physical pages used by system
       	     reduce size of buffer and page caches
       	     swap out system V shared memory pages
	     swapping out and discarding pages

tries to free 6 pages, and if that doesnt work 3 pages every time it runs and threshold is violated

wakes more frequently depending on memory pressure

page and buffercaches are good candidates for adding to free_area because the need not be written
     to physical storage
       like swaps do
     the trick is to remove caches from all processes fairly, so the penalty is equal between proc's

depending on memory pressure, the kswapd tries to free larger blocks of pages
	  uses a cyclical buffer, attempting to free a few pages at a time

looks like shared memory has been changed and shm_id is obsolete

during swap, the page table entry of each process using shared memory must be updated to indicate that the
       physical page now resides in swap space
       finds page table by finding vm_area_struct for each process


if buffer and page caches can't be reduced then
   try to swap out system v shared mem
       then try swapping out and discarding pages

pages are sent to swap if they are dirty or cannot be accessed in some other way
      executable images are read-only and therefore can be discarded if not in use
      
kswapd selects a process to swap
       not whole process, only a small number of page tables that are not shared or locked

page aging is how the swap daemon determines whether a page hasn't been used for a while

the swap operation varies by vm_area_struct, taken from vm_ops

most of kmem_getpages seems to be accounting
     the one interesting function is alloc_pages_exact_node()
     	 "heart of zoned buddy allocator" -- read about this later -- for NUMA
	 	fulfills memory allcation from same node as requestor
     note that the book cites a use of __get_free_pages() which no longer exists

kmem_freepages -- frees slabs through lower-level free_pages()

the point of slab is to avoid allocs and frees, so these previous functions
    are only called when absolutely necessary

slab layer allows for creation of new caches, and the allocation and freeing of objects within
     exported to entire kernel
     it manages all of the complex allocation of extra slabs


create a new cache with kmem_cache_create

flags for a cache_create:

SLAB_HWCACHE_ALIGN
	align each object to a cache line
	      prevents objects sharing a cache line
	      improves speed but wastes memory
	      	       makes sense for frequently used slab caches, otherwise no
SLAB_POISON
	fills slab with a known value, used to catch accesses to uninitialized memory

SLAB_RED_ZONE
	inserts red zones around allocated area to detect buffer overruns

SLAB_PANIC
	panic if allocation fails
	      used where allocation must not fail

SLAB_CACHE_DMA
	allocate each slab in DMA-able memory

ctor is a constructor for the cache -- NULL if not needed, which is the likely case

destroy a cache with kmem_cache_destroy
	might sleep
	cache must be empty
	lock maybe?

get an object through kmem_cache_alloc
    if no objects available, slab allocator will use kmem_getpages()
    
free an object with kmem_cache_free

since the kernel stack is very small (8KB or 16KB) we try to limit use of the stack
      since 2.6 there is the option to reduce kernel stack to 4KB but
      	    interrupts use their own stack
      unbounded recursion and alloca() are not allowed?

since stack usage must be limited, automatic variables must be limited to a couple hundred bytes
      stack overflows occur silently!!

moral of the story is to use dynamic memory allocations whenever possible

Through the following discussion of high memory mappings, keep in mind that the kernel address space
	uses physical, not virtual, addresses

therefore, pages obtained through alloc_pages() with __GFP_HIGHMEM flag may not have a logical address








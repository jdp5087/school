###1.9###

The first procedure definition describes a linear recursive process:

(define (+ a b)
   (if (= a 0)
   b
   (inc (+ (dec a) b))))

(+ 4 5)
(inc (+ 3 5))
(inc (inc (+ 2 5)))
(inc (inc (inc (+ 1 5))))
(inc (inc (inc (inc (+ 0 5)))))
(inc (inc (inc (inc 5))))
(inc (inc (inc 6)))
(inc (inc 7))
(inc 8)
9

the second procedure describes a linear iterative process

(define (+ a b)
   (if (= a 0)
   b
   (+ (dec a) (inc b))))

(+ 4 5)
(+ 3 6)
(+ 2 7)
(+ 1 8)
(+ 0 9)
9

###1.10###

(define (A x y)
  (cond ((= y 0) 0)
        ((= x 0) (* 2 y))
        ((= y 1) 2)
        (else (A (- x 1)
                 (A x (- y 1))))))
(A 1 10)
(A 0
   (A 1 9))
(A 0
   (A 0
      (A 1 8)))
(A 0
   (A 0
      (A 0
	 (A 1 7))))
;;; until...
(A 0
   (A 0
      (A 0
	 (A 0
	    (A 0
	       (A 0
		  (A 0
		     (A 0
			(A 0
			   (A 1 1))))))))))
;;; which returns 2
;;; this doubles 2 9 times (which is 2^10)
;;; this evaluates to 1024

(A 2 4)
(A 1
   (A 2 3))
(A 1
   (A 1
      (A 2 2)))
(A 1
   (A 1
      (A 1
	 (A 2 1))))
;;; This returns 2
(A 1
   (A 1
      (A 1
	 2)))
(A 1
   (A 1
      (A 0
	 (A 1 1))))
(A 1
   (A 1
      (A 0
	 2)))
(A 1
   (A 1
      4))
(A 1
   (A 0
      (A 1 3)))
;;; ...until

(A 1
   (A 0
      (A 0
	 (A 0 1))))
; which is 2^4

(A 1 16)
;;; which is 2^16
;;; that results in 65536


(A 3 3)
(A 2
   (A 3 2))
(A 2
   (A 2
      (A 3 1)))
(A 2
   (A 2
      2))
(A 2
   (A 2
      2))
(A 2
   (A 1
      (A 2 1)))
(A 2
   (A 1
      2))
(A 2
   (A 0
      (A 1 1)))
(A 2
   (A 0
      2))
(A 2
   4)
;;; which we know returns 65536

f = 2n

g(n) = 2^n where n >= 1
g(1) = 0

I had to look up the answer for this last one. The reason that I was having trouble is because I was looking for a clean mathematical formula instead of a
recursive functional definition.

Here's what I did manage to notice.

I noticed that the pattern was:

n    h(n)     which is:
0    0        ?
1    2        2
2    4        2^2
3    16       2^4
4    65536    2^16

so I knew that the exponent was growing exponentially, but there isn't a way to express this without a piecewise function that I could think of, so I
got stuck.

Here's the actual function definition.

h(0) = 0
h(1) = 2
h(n) where n > 1 = 2^(h(n-1))

so then we have:

h(2) = 2^2
h(3) = 2^(2^2)
h(4) = 2^(2^(2^2))

###1.11###

;;; linear recursive process

(define (f n)
  (if (< n 3)
      n
      (+
       (f (- n 1))
       (* 2 (f (- n 2)))
       (* 3 (f (- n 3))))))

;;; linear iterative process

(define (g-iter x y z count n)
  (cond ((< n 3) n)
	((< count 3) x)
	(else (g-iter (+ x (* 2 y) (* 3 z))
		      x
		      y
		      (- count 1)
		      n))))

(define (g n)
  (g-iter 2 1 0 n n))


###1.12###


(define (pascal row column)
   (cond ((or (= column row) (= 1 column)) 1)
	 (else (+ (pascal (- row 1) (- column 1))
		  (pascal (- row 1) column)))))

(pascal 4 3)

###1.13###

Will fill in later when I figure out LaTeX.

###1.14###

Drawing on paper. It appears that the growth in use of space is linear (each node only needs to know about the nodes above it). The growth in time is 2^n. This is by far an upper bound, because many of the branches don't propogate full binary trees, however the growth still exceeds n2 for greater values of n. (count-change 1000) didn't return an answer within 1 minute before I aborted the process.

###1.20###

(gcd 206 40)

(if (= 40 0)
    206
    (gcd 40 (remainder 206 40)))

(gcd 40 (remainder 206 40))

(if (= (remainder 206 40) 0)
    40
    (gcd (remainder 206 40) (remainder 40 (remainder 206 40))))

;;; 1 call to remainder, 1 total

(if (= 6 0)
    40
    (gcd (remainder 206 40) (remainder 40 (remainder 206 40))))

(gcd (remainder 206 40) (remainder 40 (remainder 206 40))))

(if (= (remainder 40 (remainder 206 40)) 0)
    (remainder 206 40)
    (gcd (remainder 40 (remainder 206 40)) (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))))

;;; 2 calls to remainder, 3 total.

(if (= 4 0)
    (remainder 206 40)
    (gcd (remainder 40 (remainder 206 40)) (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))))

(gcd (remainder 40 (remainder 206 40)) (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))))

(if (= (remainder (remainder 206 40) (remainder 40 (remainder 206 40))) 0)
    (remainder 40 (remainder 206 40))
    (gcd (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))
	 (remainder (remainder 40 (remainder 206 40))
		    (remainder (remainder 206 40) (remainder 40 (remainder 206 40))))))

;;; 4 calls to remainder, 7 total

(if (= 2 0)
    (remainder 40 (remainder 206 40))
    (gcd (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))
	 (remainder (remainder 40 (remainder 206 40))
		    (remainder (remainder 206 40) (remainder 40 (remainder 206 40))))))

(gcd (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))
     (remainder (remainder 40 (remainder 206 40))
		(remainder (remainder 206 40) (remainder 40 (remainder 206 40)))))

(if (= (remainder (remainder 40 (remainder 206 40))
		  (remainder (remainder 206 40) (remainder 40 (remainder 206 40))))
       0)
    (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))
    (gcd (remainder (remainder 40 (remainder 206 40))
		    (remainder (remainder 206 40) (remainder 40 (remainder 206 40))))
	 (remainder
	  (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))
	  (remainder (remainder 40 (remainder 206 40))
		     (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))))))

;;; 7 calls to remainder, 14 total.

(if (= 0 0)
    (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))
    (gcd (remainder (remainder 40 (remainder 206 40))
		    (remainder (remainder 206 40) (remainder 40 (remainder 206 40))))
	 (remainder
	  (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))
	  (remainder (remainder 40 (remainder 206 40))
		     (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))))))

(remainder (remainder 206 40) (remainder 40 (remainder 206 40)))

;;; 4 calls to remainder, 18 total.

This was using the normal-order evaluation model. In applicative order evaluation, the remainder operation would be applied to the operands
before being substituted in the procedure body as parameters. Lets see what that looks like.

(gcd 206 40)

(if (= 40 0)
    206
    (gcd 40 (remainder 206 40)))

(gcd 40 (remainder 206 40)))

;;; 1 call to remainder, 1 total

(gcd 40 6)

(if (= 6 0)
    6
    (gcd 6 (remainder 40 6)))

(gcd 6 (remainder 40 6)))

;;; 1 call to remainder, 2 total

(gcd 6 4)

(if (= 4 0)
    6
    (gcd 4 (remainder 6 4)))

(gcd 4 (remainder 6 4))

;;; 1 call to remainder, 3 total

(gcd 4 2)

(if (= 2 0)
    4
    (gcd 2 (remainder 4 2)))

(gcd 2 (remainder 4 2))

;;; 1 call to remainder, 4 total

(gcd 2 0)

(if (= 0 0)
    2
    (gcd 0 (remainder 2 0)))

2

So we compare 18 calls to remainder in normal-order-evaluation, and 4 calls to remainder in applicative-order-evaluation.

###1.21###

(define (smallest-divisor n)
  (find-divisor n 2))
(define (find-divisor n test-divisor)
  (cond ((> (square test-divisor) n) n)
        ((divides? test-divisor n) test-divisor)
        (else (find-divisor n (+ test-divisor 1)))))
(define (divides? a b)
  (= (remainder b a) 0))


(smallest-divisor 199)
;Value: 199

(smallest-divisor 1999)
;Value: 1999

(smallest-divisor 19999)
;Value: 7

###1.22###

(define (timed-prime-test n)
  (newline)
  (display n)
  (start-prime-test n (runtime)))
(define (start-prime-test n start-time)
  (if (prime? n)
      (report-prime (- (runtime) start-time))
      0))
(define (report-prime elapsed-time)
  (display " *** ")
  (display elapsed-time)
  1)

(define (search-for-primes-iter range found)
  (if (>= found 3)
      (finish-up)
      (run-timed-prime-test range found)))

(define (finish-up)
  (newline)
  (display "All Done!"))

(define (run-timed-prime-test n f)
  (search-for-primes-iter
   (+ n 1)
   (+ (timed-prime-test n) f)))

(search-for-primes-iter 10000000000000 0)

(define (search-for-primes range)
  (search-for-primes-iter range 0))

*Note that in all of these answers I just picked out the outputs of interest from all of the other numbers*
*Also note that for input values of smaller than 1 billion, the running times were so fast that the output was*
*either 0, or so small that rounding errors were occuring*

(search-for-primes 1000000000)

1000000007 *** .04999999999998295
1000000009 *** .05000000000001137
1000000021 *** .04999999999998295

(search-for-primes 10000000000)

10000000019 *** .37999999999999545
10000000033 *** .2699999999999818
10000000061 *** .2799999999999727

(search-for-primes 100000000000)

100000000003 *** .5699999999999932
100000000019 *** .5699999999999932
100000000057 *** .589999999999975

(search-for-primes 1000000000000)

1000000000039 *** 2.150000000000034
1000000000061 *** 1.8000000000000114
1000000000063 *** 1.9599999999999795

So, sqrt(10) is about 3.16, so yes, we see approximately a sqrt(10) increase between each order of magnitude run in the test.
This does indeed suggest that the notion that programs run in time proportional to the number of steps required for the computation.
This is, of course, a simplified model of the way that instructions are executed in the computer, but the Random Access Model of computing
is generally accepted as useful in the analysis of algorithms (according to chapter 1 of CLRS).

###1.23###

(search-for-primes 1000000000)

1000000007 *** 2.9999999999972715e-2
1000000009 *** .03000000000002956
1000000021 *** .03000000000002956

(search-for-primes 10000000000)

10000000019 *** .10000000000002274
10000000033 *** .12000000000000455
10000000061 *** .1099999999999568

(search-for-primes 100000000000)

100000000003 *** .6499999999999773
100000000019 *** .36000000000001364
100000000057 *** .3599999999999568

(search-for-primes 1000000000000)

1000000000039 *** 1.1100000000000136
1000000000061 *** 1.1400000000000432
1000000000063 *** 1.1099999999999568


For the "smaller" input values of 1 billion and 10 billion, the procedures had very similar running times, with the new
smallest-divisor algrithm maybe contributing to a slight edge. However, with larger inputs of n, this change seems to have actually realized
close to half (more like 2/3) of the running time necessary than previously.

A reason that we see more of a difference in larger values of n could be because at low values, the peripheral instructions that
we normally ignore in algorithmic analysis could have more bearing on the time it takes for the the algorithm to run,
whereas with larger numbers, the core parts of the procedure that are repeated a huge number of times have more bearing on the overall running time.

Another contributor for the difference in running times could be non-obvious, like background procedures running on the computer
so that the processor is multi-tasking.

###1.24###

(define (expmod base exp m)
  (cond ((= exp 0) 1)
        ((even? exp)
         (remainder (square (expmod base (/ exp 2) m))
                    m))
        (else
         (remainder (* base (expmod base (- exp 1) m))
                    m))))
(define (fermat-test n)
  (define (try-it a)
    (= (expmod a n n) a))
  (try-it (+ 1 (random (- n 1)))))
(define (fast-prime? n times)
  (cond ((= times 0) true)
        ((fermat-test n) (fast-prime? n (- times 1)))
        (else false)))
(define (timed-prime-test n)
  (newline)
  (display n)
  (start-prime-test n (runtime)))
(define (start-prime-test n start-time)
  (if (fast-prime? n 5)
      (report-prime (- (runtime) start-time))
      0))
(define (report-prime elapsed-time)
  (display " *** ")
  (display elapsed-time)
  1)
(define (search-for-primes-iter range found)
  (if (>= found 3)
      (finish-up)
      (run-timed-prime-test range found)))
(define (finish-up)
  (newline)
  (display "All Done!"))
(define (run-timed-prime-test n f)
  (search-for-primes-iter
   (+ n 1)
   (+ (timed-prime-test n) f)))
(define (search-for-primes range)
  (search-for-primes-iter range 0))

(search-for-primes 1000000000)
(search-for-primes 1000000000000)
(search-for-primes 1000000000000000)
(search-for-primes 1000000000000000000)
(search-for-primes 1000000000000000000000)
(search-for-primes 100000000000000000000000000000000000000000000000000000000000000000000000000)

100000000000000000000000000000000000000000000000000000000000000000000000207 *** 9.999999999990905e-3
100000000000000000000000000000000000000000000000000000000000000000000000363 *** 0.
100000000000000000000000000000000000000000000000000000000000000000000000531 *** 9.999999999990905e-3

Well, mathematically speaking, log(1000) is 3, and log(1000000) is 6, so the running would be almost indistinguishable. This is evidenced
by my output data, where I had to input a number that was 10^(75) before I had any measureable running time. Truly remarkable. This exercise
has fully impressed upon me the importance of a strong mathematical foundation in computer programming.


###1.25###

I'm not fully sure here, so I will expand my answer after I do some reasearch, but I would say that it fully depends on the implementation
of the remainder function. If it is faster to call the remainder of small numbers a handful of times rather than a huge number one time, then
expmod would be faster. One advantage that I do see is that if the remainder of a lesser power of a than n happens to be divisible, then the
interpreter only has to determine the remainder of 0 by a number, which could speed things up. But this all depends how remainder is implemented
under the hood.

From Bill the Lizard: his answer corroborates my own, although in more detail. First, the arithmetic on numbers that require more than 32 bits is much slower. Since this arithmetic will be occuring in not only fast-exp, but also in the call to remainder on a very large number, the performance is much slower. The reason expmod is so fast is because it is only finding remainders on values not much larger than m.

###1.26###

The reason that the algorithm has transformed from BigO(log(n)) to BigO(n) is apparent upon brief inspection. We can observe that each time
that the exponent is even, the algorithm makes two calls to expmod instead of simply squaring the result of one call to expmod. The procedure
(square (x) (* x x) merely takes one variable and returns the product of x times x. In this solution, however, we see that Louis makes two calls to
expmod every time the exponent is even. This is espcially damaging because at most, the exponent will satisfy the else condition twice
(once at the beginning with an odd number, and once when 2/2 = 1). Thus the majority of calls to expmod will satisfy the even? condition on large
inputs where efficiency is of more importance. If we explore the behavior of this process, we will see that every time even? is true, there will be an additional two calls to expmod, so if it is the second time even evaluates to true, we have 2*2 calls to expmod, and on the third 2*2*2. Now we see the true nature of this bug, because we are looking at a 2^n process within a log(n) process. So, the magnitude is BigO(log(2^n)), or simply BigO(n).

